import os
import cv2
import logging
import torch
import shutil
import numpy as np
from PIL import Image
from config import Config
from concurrent.futures import ThreadPoolExecutor
import threading

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VideoService:
    _instance_lock = threading.Lock()
    
    def __init__(self):
        self.vl_model = None
        self.vl_processor = None
        self.audio_model = None
        logger.info("â³ VideoService (CPU æé€Ÿä¼˜åŒ–ç‰ˆ) å·²å®ä¾‹åŒ–...")

    def _load_models_if_needed(self):
        if self.vl_model is not None:
            return

        with self._instance_lock:
            if self.vl_model is not None: return

            print("\n" + "="*50)
            print("ğŸš€ [VideoService] æ­£åœ¨åŠ è½½æ¨¡å‹...")
            print("ğŸ”¥ æ£€æµ‹åˆ°å¤šæ ¸ CPUï¼Œæ­£åœ¨åº”ç”¨æ¨ç†åŠ é€Ÿç­–ç•¥...")
            
            # ğŸš€ ç­–ç•¥1: é™åˆ¶ Torch çº¿ç¨‹æ•°ï¼Œé¿å…è¿‡å¤šçº¿ç¨‹å¯¼è‡´ä¸Šä¸‹æ–‡åˆ‡æ¢å¼€é”€
            torch.set_num_threads(16) 
            
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(current_dir)
            model_cache_path = os.path.join(project_root, "model_cache")

            try:
                from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
                from faster_whisper import WhisperModel
                
                # 1. åŠ è½½ Qwen2-VL (è§†è§‰)
                print(f"   1/2 æ­£åœ¨åŠ è½½è§†è§‰æ¨¡å‹ ({Config.VISION_MODEL_ID})...")
                self.vl_model = Qwen2VLForConditionalGeneration.from_pretrained(
                    Config.VISION_MODEL_ID,
                    dtype=torch.float32, # âœ… ä¿®å¤ï¼šä½¿ç”¨ correct å‚æ•°å dtype
                    device_map="cpu",
                    cache_dir=model_cache_path,
                    low_cpu_mem_usage=True
                ).eval()
                
                self.vl_processor = AutoProcessor.from_pretrained(
                    Config.VISION_MODEL_ID,
                    cache_dir=model_cache_path
                )

                # 2. åŠ è½½ Whisper (å¬è§‰)
                print(f"   2/2 æ­£åœ¨åŠ è½½è¯­éŸ³æ¨¡å‹ (Faster-Whisper)...")
                self.audio_model = WhisperModel(
                    Config.AUDIO_MODEL_SIZE, 
                    device="cpu", 
                    compute_type="int8", # CPU ä¸Š Int8 é‡åŒ–æ˜¯å¿…é¡»çš„
                    cpu_threads=16,      
                    download_root=os.path.join(model_cache_path, "whisper") 
                )
                
                print("âœ… æ¨¡å‹åŠ è½½å®Œæ¯•ï¼")
            except Exception as e:
                logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                raise e
            print("="*50 + "\n")

    def extract_audio_text(self, video_path):
        from moviepy.editor import VideoFileClip
        if not self.audio_model: return ""
        
        logger.info("ğŸ¤ [Whisper] æ­£åœ¨æå–è¯­éŸ³...")
        try:
            audio_path = video_path + ".mp3"
            video = VideoFileClip(video_path)
            if video.audio is None:
                video.close()
                return "ï¼ˆè¯¥è§†é¢‘æ— éŸ³è½¨ï¼‰"
            
            video.audio.write_audiofile(audio_path, verbose=False, logger=None)
            video.close()
            
            segments, info = self.audio_model.transcribe(
                audio_path, 
                beam_size=5, 
                language="zh", 
                vad_filter=True 
            )
            
            text_content = ""
            for segment in segments:
                start = int(segment.start)
                end = int(segment.end)
                text_content += f"[{start}s->{end}s] {segment.text}\n"
            
            if os.path.exists(audio_path): os.remove(audio_path)
            return text_content
        except Exception as e:
            logger.error(f"è¯­éŸ³æå–å‡ºé”™: {e}")
            return f"è¯­éŸ³æå–å¤±è´¥: {e}"

    def analyze_frames(self, video_path):
        """ä½¿ç”¨ Qwen2-VL è¿›è¡Œæ™ºèƒ½æŠ½å¸§ä¸åˆ†æ"""
        from qwen_vl_utils import process_vision_info
        
        if not self.vl_model: return ""

        logger.info(f"ğŸ‘ï¸ [Qwen2-VL] å¼€å§‹æ™ºèƒ½è§†è§‰åˆ†æ...")
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS) or 24
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        descriptions = []
        frame_count = 0
        last_analysis_time = -999
        prev_frame_gray = None
        
        # ğŸš€ ç­–ç•¥2: æ™ºèƒ½è·³å¸§é€»è¾‘
        # åªæœ‰å½“ç”»é¢å˜åŒ–æ˜¾è‘— æˆ– è·ç¦»ä¸Šæ¬¡åˆ†æè¶…è¿‡ä¸€å®šæ—¶é—´(æ¯”å¦‚8ç§’) æ‰è¿›è¡Œåˆ†æ
        # æœ€å°åˆ†æé—´éš”è®¾ç½®ä¸º 2 ç§’ï¼Œé˜²æ­¢å¤ªé¢‘ç¹
        min_interval = 2.0 
        max_interval = Config.VIDEO_FRAME_INTERVAL # é»˜è®¤ 8ç§’
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret: break
            
            curr_time = frame_count / fps
            
            # 1. å¿«é€Ÿè·³è¿‡ï¼šå¦‚æœè·ç¦»ä¸Šæ¬¡åˆ†æè¿˜ä¸åˆ°æœ€å°é—´éš”ï¼Œç›´æ¥è·³è¿‡ï¼Œè¿ resize éƒ½ä¸åš
            if curr_time - last_analysis_time < min_interval:
                frame_count += 1
                continue

            # 2. åœºæ™¯å˜åŒ–æ£€æµ‹
            # å°†ç”»é¢ç¼©å°åˆ° 64x64 è¿›è¡Œå¿«é€Ÿç°åº¦å¯¹æ¯”
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            gray_small = cv2.resize(gray, (64, 64))
            
            is_scene_change = False
            if prev_frame_gray is not None:
                # è®¡ç®—ä¸¤å¸§çš„å·®å¼‚ç¨‹åº¦
                diff_score = cv2.absdiff(prev_frame_gray, gray_small).mean()
                # é˜ˆå€¼ 30ï¼šè¡¨ç¤ºç”»é¢æœ‰æ˜æ˜¾å˜åŒ–ï¼ˆåŠ¨ä½œã€åˆ‡æ¢PPTç­‰ï¼‰
                if diff_score > 30: 
                    is_scene_change = True
            else:
                is_scene_change = True # ç¬¬ä¸€å¸§å¿…åš

            # 3. å†³å®šæ˜¯å¦åˆ†æ
            # æ¡ä»¶ï¼š(è¶…è¿‡æœ€å¤§ç­‰å¾…æ—¶é—´) OR (ç”»é¢å‘ç”Ÿäº†å‰§çƒˆå˜åŒ–)
            if (curr_time - last_analysis_time >= max_interval) or is_scene_change:
                
                timestamp = int(curr_time)
                # ğŸš€ ç­–ç•¥3: æš´åŠ›å‹ç¼©å›¾ç‰‡å°ºå¯¸
                # é™åˆ¶æœ€å¤§è¾¹é•¿ä¸º 448ï¼Œå¤§å¹…å‡å°‘ Token æ•°é‡ï¼ŒCPU æ¨ç†æé€Ÿ 3-5 å€
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                h, w, _ = frame_rgb.shape
                target_size = 448
                scale = target_size / max(h, w)
                if scale < 1:
                    new_w, new_h = int(w * scale), int(h * scale)
                    frame_rgb = cv2.resize(frame_rgb, (new_w, new_h))
                
                pil_img = Image.fromarray(frame_rgb)
                
                messages = [{
                    "role": "user",
                    "content": [
                        {"type": "image", "image": pil_img},
                        {"type": "text", "text": "ç®€è¦æè¿°ç”»é¢ä¸­çš„å…³é”®æ–‡å­—æ ‡é¢˜ã€äººç‰©åŠ¨ä½œæˆ–ç¯å¢ƒå˜åŒ–ã€‚"}
                    ]
                }]
                
                try:
                    text = self.vl_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                    image_inputs, video_inputs = process_vision_info(messages)
                    inputs = self.vl_processor(
                        text=[text],
                        images=image_inputs,
                        videos=video_inputs,
                        padding=True,
                        return_tensors="pt"
                    )
                    inputs = inputs.to("cpu")
                    
                    # ğŸš€ ç­–ç•¥4: å‡å°‘ç”Ÿæˆé•¿åº¦ (max_new_tokens 128 è¶³å¤Ÿæè¿°ç”»é¢)
                    generated_ids = self.vl_model.generate(**inputs, max_new_tokens=128)
                    generated_ids_trimmed = [
                        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                    ]
                    output_text = self.vl_processor.batch_decode(
                        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
                    )[0]
                    
                    desc_line = f"[{timestamp}ç§’]: {output_text}"
                    print(desc_line)
                    descriptions.append(desc_line)
                    
                    # æ›´æ–°çŠ¶æ€
                    last_analysis_time = curr_time
                    prev_frame_gray = gray_small
                    
                except Exception as e:
                    logger.warning(f"å¸§åˆ†æå‡ºé”™: {e}")

            frame_count += 1
        
        cap.release()
        return "\n".join(descriptions)

    def process_video(self, video_path):
        self._load_models_if_needed()
        logger.info(f"ğŸ¬ å¼€å§‹å¹¶è¡Œå¤„ç†: {os.path.basename(video_path)}")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†è§†è§‰å’Œå¬è§‰
        with ThreadPoolExecutor(max_workers=2) as executor:
            future_vision = executor.submit(self.analyze_frames, video_path)
            future_audio = executor.submit(self.extract_audio_text, video_path)
            
            visual_desc = future_vision.result()
            audio_text = future_audio.result()
        
        final_report = f"""
# è§†é¢‘å¤šæ¨¡æ€åˆ†ææŠ¥å‘Š
æ–‡ä»¶å: {os.path.basename(video_path)}
åˆ†æç­–ç•¥: æ™ºèƒ½å…³é”®å¸§æ£€æµ‹ + è¯­éŸ³è½¬å½•

## 1. è§†è§‰ç”»é¢è®°å½•
{visual_desc}

## 2. è¯­éŸ³è½¬å½•å†…å®¹
{audio_text}
"""
        logger.info("âœ… è§†é¢‘æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return final_report

_video_service = None
def get_video_service():
    global _video_service
    if _video_service is None:
        _video_service = VideoService()
    return _video_service