import os
import cv2
import logging
import torch
import shutil
import numpy as np
import threading
import multiprocessing
from PIL import Image
from config import Config
from concurrent.futures import ThreadPoolExecutor
from qwen_vl_utils import process_vision_info # ç¡®ä¿å·²å®‰è£… qwen-vl-utils

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VideoService:
    _instance_lock = threading.Lock()
    
    def __init__(self):
        self.vl_model = None
        self.vl_processor = None
        self.audio_model = None
        logger.info("â³ VideoService (æ‰¹å¤„ç†+é‡åŒ–ä¼˜åŒ–ç‰ˆ) å·²å®ä¾‹åŒ–...")

    def _load_models_if_needed(self):
        if self.vl_model is not None:
            return

        with self._instance_lock:
            if self.vl_model is not None: return

            print("\n" + "="*50)
            print("ğŸš€ [VideoService] æ­£åœ¨åŠ è½½æ¨¡å‹...")
            
            # ğŸš€ ä¼˜åŒ–1: åŠ¨æ€çº¿ç¨‹
            total_cores = multiprocessing.cpu_count()
            compute_threads = max(1, total_cores - 4) 
            torch.set_num_threads(compute_threads)
            print(f"ğŸ”¥ æ£€æµ‹åˆ° {total_cores} æ ¸ CPUï¼Œå·²åˆ†é… {compute_threads} ä¸ªè®¡ç®—çº¿ç¨‹")
            
            model_cache_path = Config.MODEL_CACHE_DIR

            try:
                from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
                from faster_whisper import WhisperModel
                
                # 1. åŠ è½½ Qwen2-VL
                print(f"   1/2 æ­£åœ¨åŠ è½½è§†è§‰æ¨¡å‹ ({Config.VISION_MODEL_ID})...")
                model = Qwen2VLForConditionalGeneration.from_pretrained(
                    Config.VISION_MODEL_ID,
                    dtype=torch.float32, 
                    device_map="cpu",
                    cache_dir=model_cache_path,
                    low_cpu_mem_usage=True
                )
                
                # ğŸš€ ä¼˜åŒ–2: åŠ¨æ€é‡åŒ– (Int8)
                print("   âš¡ æ­£åœ¨åº”ç”¨ CPU åŠ¨æ€é‡åŒ– (Int8)...")
                self.vl_model = torch.quantization.quantize_dynamic(
                    model, {torch.nn.Linear}, dtype=torch.qint8
                )
                self.vl_model.eval()
                
                self.vl_processor = AutoProcessor.from_pretrained(
                    Config.VISION_MODEL_ID,
                    cache_dir=model_cache_path
                )

                # 2. åŠ è½½ Whisper
                print(f"   2/2 æ­£åœ¨åŠ è½½è¯­éŸ³æ¨¡å‹ (Faster-Whisper)...")
                self.audio_model = WhisperModel(
                    Config.AUDIO_MODEL_SIZE, 
                    device="cpu", 
                    compute_type="int8", 
                    cpu_threads=4,      
                    download_root=os.path.join(model_cache_path, "whisper") 
                )
                
                print("âœ… æ¨¡å‹åŠ è½½ä¸ä¼˜åŒ–å®Œæ¯•ï¼")
            except Exception as e:
                logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                raise e
            print("="*50 + "\n")

    def extract_audio_text(self, video_path):
        from moviepy.editor import VideoFileClip
        if not self.audio_model: return ""
        
        logger.info("ğŸ¤ [Whisper] æ­£åœ¨æå–è¯­éŸ³...")
        try:
            audio_path = video_path + ".mp3"
            video = VideoFileClip(video_path)
            if video.audio is None:
                video.close()
                return "ï¼ˆè¯¥è§†é¢‘æ— éŸ³è½¨ï¼‰"
            
            video.audio.write_audiofile(audio_path, verbose=False, logger=None)
            video.close()
            
            segments, info = self.audio_model.transcribe(
                audio_path, 
                beam_size=5, 
                language="zh", 
                vad_filter=True 
            )
            
            text_content = ""
            for segment in segments:
                start = int(segment.start)
                end = int(segment.end)
                text_content += f"[{start}s->{end}s] {segment.text}\n"
            
            if os.path.exists(audio_path): os.remove(audio_path)
            return text_content
        except Exception as e:
            logger.error(f"è¯­éŸ³æå–å‡ºé”™: {e}")
            return f"è¯­éŸ³æå–å¤±è´¥: {e}"

    def analyze_frames(self, video_path):
        """ğŸš€ æ ¸å¿ƒé‡æ„ï¼šæ”¯æŒ Batch å¤„ç†ä¸é«˜æ¸…åˆ†æ"""
        if not self.vl_model: return ""

        logger.info(f"ğŸ‘ï¸ [Qwen2-VL] å¼€å§‹æ™ºèƒ½æ‰¹å¤„ç†è§†è§‰åˆ†æ...")
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS) or 24
        
        descriptions = []
        
        # æ‰¹å¤„ç† Buffer
        batch_frames = []      # å­˜ PIL Image
        batch_timestamps = []  # å­˜ æ—¶é—´æˆ³
        
        frame_count = 0
        last_analysis_time = -999
        prev_frame_gray = None
        
        min_interval = 2.0 
        max_interval = Config.VIDEO_FRAME_INTERVAL
        
        # ğŸš€ ä¼˜åŒ–3: æ‰¹å¤„ç†å¾ªç¯
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret: break
            
            curr_time = frame_count / fps
            
            # 1. å¿«é€Ÿè·³è¿‡é€»è¾‘
            if curr_time - last_analysis_time < min_interval:
                frame_count += 1
                continue

            # 2. åœºæ™¯å˜åŒ–æ£€æµ‹ (ç°åº¦å°å›¾)
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            gray_small = cv2.resize(gray, (64, 64))
            
            is_scene_change = False
            if prev_frame_gray is not None:
                diff_score = cv2.absdiff(prev_frame_gray, gray_small).mean()
                if diff_score > 30: is_scene_change = True
            else:
                is_scene_change = True

            # 3. å†³å®šæ˜¯å¦å…¥é˜Ÿ
            if (curr_time - last_analysis_time >= max_interval) or is_scene_change:
                timestamp = int(curr_time)
                
                # ğŸš€ ä¼˜åŒ–4: ç”»è´¨æå‡ï¼Œä¸å†å¼ºåˆ¶ resize åˆ° 448
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_img = Image.fromarray(frame_rgb)
                
                batch_frames.append(pil_img)
                batch_timestamps.append(timestamp)
                
                last_analysis_time = curr_time
                prev_frame_gray = gray_small
                
                # 4. æ‰¹å¤„ç†è§¦å‘ï¼šå½“ç§¯æ”’åˆ° BATCH_SIZE (8å¸§) æ—¶ï¼Œä¸€æ¬¡æ€§å‘é€ç»™ CPU
                if len(batch_frames) >= Config.VIDEO_BATCH_SIZE:
                    self._process_batch(batch_frames, batch_timestamps, descriptions)
                    batch_frames = []
                    batch_timestamps = []

            frame_count += 1
        
        # å¤„ç†å‰©ä½™çš„å°¾å¸§
        if batch_frames:
            self._process_batch(batch_frames, batch_timestamps, descriptions)
        
        cap.release()
        return "\n".join(descriptions)

    def _process_batch(self, images, timestamps, descriptions):
        """å†…éƒ¨æ–¹æ³•ï¼šæ‰§è¡Œ Batch æ¨ç†"""
        try:
            print(f"âš¡ [Batch] æ­£åœ¨å¹¶è¡Œå¤„ç† {len(images)} å¸§...")
            
            # æ„é€  Batch Prompt
            messages_batch = []
            system_instruction = "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è§†é¢‘åˆ†æå‘˜ã€‚è¯·å®¢è§‚æè¿°ç”»é¢ï¼Œä¸è¦çŒœæµ‹ï¼Œä¸è¦ç¼–é€ å†…å®¹ã€‚å¦‚æœæ–‡å­—æ¨¡ç³Šï¼Œå°±è¯´æ— æ³•è¯†åˆ«ã€‚"
            for img in images:
                messages_batch.append([
                    {
                        "role": "user",
                        "content": [
                            # Qwen2-VL ä¼šè‡ªåŠ¨å¤„ç† resizeï¼Œæˆ‘ä»¬åªéœ€æ§åˆ¶ max_pixels
                            {"type": "image", "image": img, "max_pixels": Config.VIDEO_MAX_PIXELS},
                            {"type": "text", "text": f"{system_instruction}\nç®€è¦æè¿°ç”»é¢ä¸­çš„å…³é”®æ–‡å­—æ ‡é¢˜ã€äººç‰©åŠ¨ä½œæˆ–ç¯å¢ƒå˜åŒ–ã€‚"}
                        ]
                    }
                ])
            
            # é¢„å¤„ç†
            texts = [
                self.vl_processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)
                for msg in messages_batch
            ]
            
            image_inputs, video_inputs = process_vision_info(messages_batch)
            
            # è¿™é‡Œçš„ batching å‘ç”Ÿåœ¨ inputs æ„å»ºé˜¶æ®µ
            inputs = self.vl_processor(
                text=texts,
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt"
            )
            inputs = inputs.to("cpu") # ç¡®ä¿åœ¨ CPU
            
            # æ¨ç†
            # Batch generate: è¾“å…¥å¼ é‡å·²ç»æ˜¯ [Batch, ...] ç»´åº¦
            generated_ids = self.vl_model.generate(**inputs, max_new_tokens=128)
            
            # è§£ç 
            generated_ids_trimmed = [
                out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_texts = self.vl_processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )
            
            # ç»“æœå›å¡«
            for i, text in enumerate(output_texts):
                desc_line = f"[{timestamps[i]}ç§’]: {text}"
                print(desc_line)
                descriptions.append(desc_line)
                
        except Exception as e:
            logger.error(f"âŒ Batch æ¨ç†å¤±è´¥: {e}")

    def process_video(self, video_path):
        self._load_models_if_needed()
        logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†: {os.path.basename(video_path)}")
        
        # è§†è§‰åˆ†æ (ç°åœ¨æ˜¯ Batch çš„)
        # æ³¨æ„ï¼šç”±äº analyze_frames å†…éƒ¨å·²ç»æ˜¯ç”¨å°½äº† CPU æ ¸å¿ƒï¼Œè¿™é‡Œå†ç”¨ ThreadPoolExecutor 
        # å’Œ audio å¹¶è¡Œå¯èƒ½ä¼šå¯¼è‡´èµ„æºäº‰æŠ¢ã€‚
        # é‰´äº audio æ¯”è¾ƒå¿«ï¼Œæˆ‘ä»¬æ”¹æˆä¸²è¡Œï¼Œæˆ–è€…è®© audio åœ¨åå°è·‘ã€‚
        # ä¸ºäº†ç¨³å®šæ€§ï¼Œè¿™é‡Œæ”¹ä¸ºç®€å•çš„ä¸²è¡Œï¼ˆå…ˆè§†è§‰åå¬è§‰ï¼Œæˆ–è€…åä¹‹ï¼‰ï¼Œ
        # å› ä¸ºè§†è§‰ç°åœ¨èƒ½åƒæ»¡ 32 æ ¸ï¼Œä¸å®œåˆ†å¿ƒã€‚
        
        visual_desc = self.analyze_frames(video_path)
        audio_text = self.extract_audio_text(video_path)
        
        final_report = f"""
# è§†é¢‘å¤šæ¨¡æ€åˆ†ææŠ¥å‘Š
æ–‡ä»¶å: {os.path.basename(video_path)}
åˆ†æç­–ç•¥: åŠ¨æ€é‡åŒ–(Int8) + 32æ ¸æ‰¹å¤„ç† + é«˜æ¸…é‡‡æ ·

## 1. è§†è§‰ç”»é¢è®°å½•
{visual_desc}

## 2. è¯­éŸ³è½¬å½•å†…å®¹
{audio_text}
"""
        logger.info("âœ… è§†é¢‘æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return final_report

_video_service = None
def get_video_service():
    global _video_service
    if _video_service is None:
        _video_service = VideoService()
    return _video_service