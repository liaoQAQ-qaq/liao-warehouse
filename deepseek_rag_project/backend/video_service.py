import os
import cv2
import logging
import torch
import multiprocessing
import shutil
from PIL import Image
from config import Config
from qwen_vl_utils import process_vision_info

# é…ç½®ç®€æ´çš„æ—¥å¿—æ ¼å¼
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VideoService:
    def __init__(self):
        self.vl_model = None
        self.vl_processor = None
        self.audio_model = None
        logger.info("VideoService Initialized.")

    def _load_models_if_needed(self):
        if self.vl_model is not None:
            return

        logger.info("Loading models...")
        
        # åŠ¨æ€è®¡ç®—çº¿ç¨‹æ•°
        total_cores = multiprocessing.cpu_count()
        compute_threads = max(1, total_cores - 4) 
        torch.set_num_threads(compute_threads)
        
        model_cache_path = Config.MODEL_CACHE_DIR

        try:
            from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
            from faster_whisper import WhisperModel
            
            # 1. åŠ è½½ Qwen2-VL (è§†è§‰)
            logger.info(f"Loading Vision Model: {Config.VISION_MODEL_ID}")
            # ğŸš€ã€æ ¸å¿ƒä¿®å¤ã€‘ç§»é™¤ quantize_dynamic
            # Qwen2-VL å¯¹ç²¾åº¦æ•æ„Ÿï¼ŒCPU ä¸Š Int8 é‡åŒ–ä¼šå¯¼è‡´â€œè‡´ç›²â€äº§ç”Ÿå¹»è§‰
            # æ‚¨çš„ 128GB å†…å­˜å®Œå…¨è¶³å¤Ÿè·‘ FP32
            self.vl_model = Qwen2VLForConditionalGeneration.from_pretrained(
                Config.VISION_MODEL_ID,
                torch_dtype=torch.float32, # æ˜ç¡®ä½¿ç”¨ FP32 ä¿è¯ç²¾åº¦
                device_map="cpu",
                cache_dir=model_cache_path,
                low_cpu_mem_usage=True
            )
            # self.vl_model.eval() # from_pretrained é»˜è®¤å°±æ˜¯ eval æ¨¡å¼
            
            self.vl_processor = AutoProcessor.from_pretrained(
                Config.VISION_MODEL_ID,
                cache_dir=model_cache_path
            )

            # 2. åŠ è½½ Whisper (å¬è§‰) - Whisper çš„ Int8 æ˜¯å®˜æ–¹æ”¯æŒçš„ï¼Œå®‰å…¨
            logger.info("Loading Audio Model: Faster-Whisper")
            self.audio_model = WhisperModel(
                Config.AUDIO_MODEL_SIZE, 
                device="cpu", 
                compute_type="int8", 
                cpu_threads=4,      
                download_root=os.path.join(model_cache_path, "whisper") 
            )
            
            logger.info("All models loaded successfully.")
        except Exception as e:
            logger.error(f"Model loading failed: {e}")
            raise e

    def extract_audio_text(self, video_path):
        from moviepy.editor import VideoFileClip
        if not self.audio_model: return ""
        
        logger.info("Starting audio transcription...")
        temp_audio_path = video_path + ".wav"

        try:
            video = VideoFileClip(video_path)
            if video.audio is None:
                video.close()
                return "ï¼ˆè¯¥è§†é¢‘æ— éŸ³è½¨ï¼‰"
            
            video.audio.write_audiofile(temp_audio_path, codec='pcm_s16le', verbose=False, logger=None)
            video.close()
            
            segments, info = self.audio_model.transcribe(
                temp_audio_path, 
                beam_size=5, 
                language="zh", 
                vad_filter=True,
                vad_parameters=dict(min_silence_duration_ms=500),
                initial_prompt="ä»¥ä¸‹æ˜¯ä¸€æ®µä¸­æ–‡ä¼šè®®è®°å½•æˆ–å¯¹è¯ï¼Œè¯·å‡†ç¡®è½¬å½•å†…å®¹ã€‚"
            )
            
            text_lines = []
            for segment in segments:
                start = int(segment.start)
                end = int(segment.end)
                text_lines.append(f"- [{start}s-{end}s]: {segment.text.strip()}")
            
            final_text = "\n".join(text_lines)
            
            if os.path.exists(temp_audio_path): 
                os.remove(temp_audio_path)
                
            return final_text if final_text else "ï¼ˆéŸ³é¢‘è½¬å½•ä¸ºç©ºï¼‰"

        except Exception as e:
            logger.error(f"Audio extraction error: {e}")
            if os.path.exists(temp_audio_path): os.remove(temp_audio_path)
            return f"è¯­éŸ³æå–å¤±è´¥: {e}"

    def analyze_frames(self, video_path):
        if not self.vl_model: return ""

        logger.info("Starting visual analysis...")
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS) or 24
        
        descriptions = []
        batch_frames = []
        batch_timestamps = []
        
        frame_count = 0
        last_analysis_time = -999
        prev_frame_gray = None
        
        min_interval = 2.0 
        max_interval = Config.VIDEO_FRAME_INTERVAL
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret: break
            
            curr_time = frame_count / fps
            
            if curr_time - last_analysis_time < min_interval:
                frame_count += 1
                continue

            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            gray_small = cv2.resize(gray, (64, 64))
            
            is_scene_change = False
            if prev_frame_gray is not None:
                diff_score = cv2.absdiff(prev_frame_gray, gray_small).mean()
                if diff_score > 30: is_scene_change = True
            else:
                is_scene_change = True

            if (curr_time - last_analysis_time >= max_interval) or is_scene_change:
                timestamp = int(curr_time)
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_img = Image.fromarray(frame_rgb)
                
                batch_frames.append(pil_img)
                batch_timestamps.append(timestamp)
                
                last_analysis_time = curr_time
                prev_frame_gray = gray_small
                
                if len(batch_frames) >= Config.VIDEO_BATCH_SIZE:
                    self._process_batch(batch_frames, batch_timestamps, descriptions)
                    batch_frames = []
                    batch_timestamps = []

            frame_count += 1
        
        if batch_frames:
            self._process_batch(batch_frames, batch_timestamps, descriptions)
        
        cap.release()
        return "\n".join(descriptions)

    def _process_batch(self, images, timestamps, descriptions):
        try:
            print(f"Processing batch of {len(images)} frames...", flush=True)
            
            messages_batch = []
            # ç®€åŒ– Promptï¼Œç¡®ä¿æ¨¡å‹èƒ½ç›´æ¥å›ç­”
            system_instruction = "Describe this image in detail."
            
            for img in images:
                messages_batch.append([
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "image": img, "max_pixels": Config.VIDEO_MAX_PIXELS},
                            {"type": "text", "text": system_instruction}
                        ]
                    }
                ])
            
            texts = [
                self.vl_processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)
                for msg in messages_batch
            ]
            
            image_inputs, video_inputs = process_vision_info(messages_batch)
            
            inputs = self.vl_processor(
                text=texts,
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt"
            )
            # ç¡®ä¿è¾“å…¥æ•°æ®ä¹Ÿåœ¨ CPU
            inputs = inputs.to("cpu")
            
            # æ¨ç†
            generated_ids = self.vl_model.generate(**inputs, max_new_tokens=128)
            
            generated_ids_trimmed = [
                out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_texts = self.vl_processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True
            )
            
            for i, text in enumerate(output_texts):
                clean_text = text.replace("\n", " ").strip()
                # å¦‚æœè¾“å‡ºä¸ºç©ºï¼Œè®°å½•è­¦å‘Š
                if not clean_text:
                    logger.warning(f"Frame at {timestamps[i]}s produced empty description.")
                    clean_text = "(æ— æ³•è¯†åˆ«ç”»é¢å†…å®¹)"
                descriptions.append(f"[{timestamps[i]}s]: {clean_text}")
                
        except Exception as e:
            logger.error(f"Batch inference failed: {e}")

    def process_video(self, video_path):
        self._load_models_if_needed()
        logger.info(f"Processing video: {os.path.basename(video_path)}")
        
        visual_desc = self.analyze_frames(video_path)
        audio_text = self.extract_audio_text(video_path)
        
        final_report = f"""
# è§†é¢‘æ™ºèƒ½åˆ†ææŠ¥å‘Š
æ–‡ä»¶å: {os.path.basename(video_path)}

## 1. è§†è§‰æ‘˜è¦ (Visual)
{visual_desc}

## 2. è¯­éŸ³è½¬å½• (Audio)
{audio_text}
"""
        return final_report

_video_service = None
def get_video_service():
    global _video_service
    if _video_service is None:
        _video_service = VideoService()
    return _video_service