import os
import cv2
import logging
import torch
import shutil
from PIL import Image
from config import Config

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VideoService:
    def __init__(self):
        self.vl_model = None
        self.vl_processor = None
        self.audio_model = None
        logger.info("â³ VideoService (Proç‰ˆ) å·²å®ä¾‹åŒ–...")

    def _load_models_if_needed(self):
        if self.vl_model is not None:
            return

        print("\n" + "="*50)
        print("ğŸš€ [VideoService] æ­£åœ¨åŠ è½½é«˜æ€§èƒ½æ¨¡å‹ (32æ ¸ CPU åŠ é€Ÿä¸­)...")
        print("   è¿™å¯èƒ½éœ€è¦ 2-3 åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        # è·å–ç»å¯¹è·¯å¾„
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(current_dir)
        model_cache_path = os.path.join(project_root, "model_cache")

        try:
            from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
            from faster_whisper import WhisperModel
            
            # 1. åŠ è½½ Qwen2-VL-7B (è§†è§‰)
            print(f"   1/2 æ­£åœ¨åŠ è½½è§†è§‰æ¨¡å‹ ({Config.VISION_MODEL_ID})...")
            self.vl_model = Qwen2VLForConditionalGeneration.from_pretrained(
                Config.VISION_MODEL_ID,
                torch_dtype=torch.float32, # CPU å¿…é¡»ç”¨ float32
                device_map="cpu",
                cache_dir=model_cache_path,
                low_cpu_mem_usage=True
            ).eval()
            
            self.vl_processor = AutoProcessor.from_pretrained(
                Config.VISION_MODEL_ID,
                cache_dir=model_cache_path
            )

            # 2. åŠ è½½ Whisper Large-v3 (å¬è§‰)
            print(f"   2/2 æ­£åœ¨åŠ è½½è¯­éŸ³æ¨¡å‹ (Faster-Whisper {Config.AUDIO_MODEL_SIZE})...")
            # æ„é€  whisper æ¨¡å‹çš„æœ¬åœ°è·¯å¾„
            # æ³¨æ„ï¼šfaster-whisper ä¸‹è½½çš„æ–‡ä»¶å¤¹åé€šå¸¸æ˜¯ "models--Systran--faster-whisper-large-v3" ä¸‹çš„ snapshots/xxx
            # è¿™é‡Œæˆ‘ä»¬è®©å®ƒè‡ªåŠ¨å» cache ç›®å½•æ‰¾ï¼Œå¦‚æœæ‰¾ä¸åˆ°ä¼šè‡ªåŠ¨ä¸‹è½½ï¼ˆä½†æˆ‘ä»¬å‰é¢å·²ç»ä¸‹è½½è¿‡äº†ï¼‰
            self.audio_model = WhisperModel(
                Config.AUDIO_MODEL_SIZE, 
                device="cpu", 
                compute_type="int8", # int8 é‡åŒ–ï¼Œåœ¨ CPU ä¸Šé€Ÿåº¦å¿«ä¸”ç²¾åº¦å‡ ä¹ä¸é™
                download_root=os.path.join(model_cache_path, "whisper") 
            )
            
            print("âœ… é¡¶é…æ¨¡å‹åŠ è½½å®Œæ¯•ï¼")
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # æ‰“å°è¯¦ç»†é”™è¯¯æ ˆ
            import traceback
            traceback.print_exc()
            raise e
        print("="*50 + "\n")

    def extract_audio_text(self, video_path):
        from moviepy.editor import VideoFileClip
        if not self.audio_model: return ""
        
        logger.info("ğŸ¤ [Whisper] æ­£åœ¨è¿›è¡Œé«˜ç²¾åº¦è¯­éŸ³è½¬å½•...")
        try:
            audio_path = video_path + ".mp3"
            video = VideoFileClip(video_path)
            if video.audio is None:
                video.close()
                return "ï¼ˆè¯¥è§†é¢‘æ— éŸ³è½¨ï¼‰"
            
            video.audio.write_audiofile(audio_path, verbose=False, logger=None)
            video.close()
            
            # beam_size=5 æå‡å‡†ç¡®ç‡
            segments, info = self.audio_model.transcribe(
                audio_path, 
                beam_size=5, 
                language="zh", # å¼ºåˆ¶ä¸­æ–‡ï¼Œæˆ–å»æ‰è‡ªåŠ¨æ£€æµ‹
                vad_filter=True # è‡ªåŠ¨è¿‡æ»¤é™éŸ³ç‰‡æ®µ
            )
            
            text_content = ""
            for segment in segments:
                # æ ¼å¼åŒ–æ—¶é—´æˆ³ [00:10 -> 00:15] æ–‡æœ¬
                start = int(segment.start)
                end = int(segment.end)
                text_content += f"[{start}s->{end}s] {segment.text}\n"
            
            if os.path.exists(audio_path): os.remove(audio_path)
            return text_content
        except Exception as e:
            logger.error(f"è¯­éŸ³æå–å‡ºé”™: {e}")
            return f"è¯­éŸ³æå–å¤±è´¥: {e}"

    def analyze_frames(self, video_path):
        """ä½¿ç”¨ Qwen2-VL è¿›è¡Œæ·±åº¦ç”»é¢ç†è§£ (è‡ªå¸¦OCR)"""
        from qwen_vl_utils import process_vision_info
        
        if not self.vl_model: return ""

        logger.info(f"ğŸ‘ï¸ [Qwen2-VL] å¼€å§‹é€å¸§æ·±åº¦åˆ†æ (é—´éš” {Config.VIDEO_FRAME_INTERVAL}ç§’)...")
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS) or 24
        interval = int(fps * Config.VIDEO_FRAME_INTERVAL)
        
        descriptions = []
        frame_count = 0
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret: break
            
            if frame_count % interval == 0:
                timestamp = int(frame_count // fps)
                pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                
                # Qwen2-VL æç¤ºè¯ï¼šè¦æ±‚å…¶åš OCR å¹¶æè¿°ç»†èŠ‚
                # 32æ ¸ CPU å¯ä»¥æ‰›å¾—ä½ç¨å¾®é•¿ä¸€ç‚¹çš„ç”Ÿæˆ
                messages = [{
                    "role": "user",
                    "content": [
                        {"type": "image", "image": pil_img},
                        {"type": "text", "text": "è¯·è¯¦ç»†æè¿°è¿™å¼ ç”»é¢çš„å†…å®¹ã€‚1. å¦‚æœæ˜¯è½¯ä»¶ç•Œé¢æˆ–æ–‡æ¡£ï¼Œè¯·å‡†ç¡®æå–ä¸Šé¢çš„æ–‡å­—æ ‡é¢˜å’Œå…³é”®å†…å®¹ã€‚2. å¦‚æœæ˜¯ç°å®åœºæ™¯ï¼Œè¯·æè¿°äººç‰©åŠ¨ä½œå’Œç¯å¢ƒç»†èŠ‚ã€‚"}
                    ]
                }]
                
                try:
                    # é¢„å¤„ç†
                    text = self.vl_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                    image_inputs, video_inputs = process_vision_info(messages)
                    inputs = self.vl_processor(
                        text=[text],
                        images=image_inputs,
                        videos=video_inputs,
                        padding=True,
                        return_tensors="pt"
                    )
                    inputs = inputs.to("cpu")
                    
                    # æ¨ç† (max_new_tokens å¯ä»¥é€‚å½“è°ƒå¤§ï¼Œå› ä¸º 7B æ¨¡å‹åºŸè¯å°‘ï¼Œæ¯”è¾ƒç²¾å‡†)
                    generated_ids = self.vl_model.generate(**inputs, max_new_tokens=256)
                    generated_ids_trimmed = [
                        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                    ]
                    output_text = self.vl_processor.batch_decode(
                        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
                    )[0]
                    
                    # å®æ—¶æ‰“å°è¿›åº¦
                    desc_line = f"[{timestamp}ç§’ç”»é¢]: {output_text}"
                    print(desc_line)
                    descriptions.append(desc_line)
                    
                except Exception as e:
                    logger.warning(f"å¸§åˆ†æå‡ºé”™: {e}")

            frame_count += 1
        
        cap.release()
        return "\n".join(descriptions)

    def process_video(self, video_path):
        self._load_models_if_needed()
        logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†: {os.path.basename(video_path)}")
        
        # 1. è§†è§‰åˆ†æ (Qwen2-VL-7B)
        visual_desc = self.analyze_frames(video_path)
        
        # 2. å¬è§‰åˆ†æ (Whisper-Large-v3)
        audio_text = self.extract_audio_text(video_path)
        
        # 3. æ±‡æ€»æŠ¥å‘Š
        final_report = f"""
# è§†é¢‘å¤šæ¨¡æ€æ·±åº¦åˆ†ææŠ¥å‘Š
æ–‡ä»¶å: {os.path.basename(video_path)}
åˆ†ææ¨¡å‹: Qwen2-VL-7B (è§†è§‰) + Whisper-Large-v3 (è¯­éŸ³)

## 1. è§†è§‰ä¸OCRåˆ†æè®°å½•
{visual_desc}

## 2. è¯­éŸ³è½¬å½•è®°å½•
{audio_text}
"""
        logger.info("âœ… è§†é¢‘æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return final_report

_video_service = None
def get_video_service():
    global _video_service
    if _video_service is None:
        _video_service = VideoService()
    return _video_service