import json
import re
from config import Config
from vector_store import get_vector_service
from prompts import get_qa_prompt_template
from llama_index.core import Settings
from llama_index.core.llms import ChatMessage, MessageRole

class RAGService:
    def __init__(self):
        self.vector_service = get_vector_service()
        self.retriever = self.vector_service.index.as_retriever(similarity_top_k=5)
        self.qa_prompt = get_qa_prompt_template()

    def format_docs_with_id(self, nodes):
        formatted_str = ""
        file_groups = {}
        
        for node in nodes:
            file_name = node.node.metadata.get("file_name", "æœªçŸ¥æ–‡ä»¶")
            score = node.score if node.score else 0.0
            content = node.node.get_content().replace('\n', ' ')
            
            if file_name not in file_groups:
                file_groups[file_name] = {"content": [], "max_score": 0.0}
            
            file_groups[file_name]["content"].append(content)
            if score > file_groups[file_name]["max_score"]:
                file_groups[file_name]["max_score"] = score

        sorted_files = sorted(file_groups.items(), key=lambda x: x[1]['max_score'], reverse=True)
        
        for idx, (file_name, data) in enumerate(sorted_files):
            combined_content = "... ".join(data["content"])
            formatted_str += f"[{idx+1}] (æ¥æº: {file_name}): {combined_content}\n\n"
            
        return formatted_str, sorted_files

    async def chat_stream(self, question: str):
        # 1. æ£€ç´¢
        nodes_with_score = await self.retriever.aretrieve(question)
        
        if not nodes_with_score:
            yield "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            return

        # 2. æ ¼å¼åŒ–
        context_str, sorted_files = self.format_docs_with_id(nodes_with_score)
        
        # 3. ç»„è£… Prompt
        fmt_prompt = self.qa_prompt.format(context_str=context_str, query_str=question)
        
        # 4. è°ƒç”¨ LLM
        messages = [ChatMessage(role=MessageRole.USER, content=fmt_prompt)]
        full_answer = ""
        
        response_gen = await Settings.llm.astream_chat(messages)
        
        async for response in response_gen:
            content = response.delta
            full_answer += content
            yield content

        # 5. å¼•ç”¨è¿‡æ»¤é€»è¾‘ (å¢åŠ å›¾ç‰‡è±å…)
        cited_indices = set()
        matches = re.findall(r'\[(\d+)\]', full_answer)
        for m in matches:
            cited_indices.add(int(m))

        source_data = []
        for idx, (file_name, data) in enumerate(sorted_files):
            current_id = idx + 1
            
            # ğŸš€ã€æ ¸å¿ƒä¼˜åŒ–ã€‘å›¾ç‰‡ç‰¹æƒé€»è¾‘
            # å¦‚æœæ˜¯å›¾ç‰‡ä¸”æ’ç¬¬ä¸€ï¼Œå³ä½¿æ²¡è¢«å¼•ç”¨ä¹Ÿæ˜¾ç¤ºï¼ˆé˜²æ­¢çœ‹å›¾è¯´è¯ä¸¢å¤±æ¥æºï¼‰
            is_image = file_name.lower().endswith(('.jpg', '.png', '.jpeg'))
            is_top_result = (idx == 0)
            
            if current_id in cited_indices:
                source_data.append({
                    "id": current_id,
                    "name": file_name,
                    "score": round(data["max_score"], 2)
                })
            elif is_image and is_top_result:
                source_data.append({
                    "id": current_id,
                    "name": file_name,
                    "score": round(data["max_score"], 2)
                })

        # 6. å‘é€ JSON
        if source_data:
            yield f"__SOURCES__{json.dumps(source_data)}"

_rag = None
def get_rag_service():
    global _rag
    if _rag is None:
        _rag = RAGService()
    return _rag