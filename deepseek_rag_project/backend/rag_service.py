import logging
from llama_index.core import VectorStoreIndex, StorageContext, Settings
from llama_index.core.llms import ChatMessage, MessageRole
from llama_index.vector_stores.milvus import MilvusVectorStore
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from config import Config
from session_manager import session_manager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RAGService:
    def __init__(self):
        logger.info("ğŸ¤– åˆå§‹åŒ– RAG æœåŠ¡ (ç›´è¿å¼ºæ§ç‰ˆ)...")
        
        try:
            logger.info(f"ğŸ”Œ åŠ è½½ Embedding: {Config.EMBEDDING_MODEL}")
            Settings.embed_model = HuggingFaceEmbedding(
                model_name=Config.EMBEDDING_MODEL,
                cache_folder=Config.MODEL_CACHE_DIR
            )
            
            logger.info(f"ğŸ§  è¿æ¥ LLM: {Config.LLM_MODEL}")
            # ğŸš€ å…³é”®é…ç½®ï¼šå¼ºåˆ¶æŒ‡å®šä¸Šä¸‹æ–‡çª—å£ï¼Œé˜²æ­¢ Empty Response
            Settings.llm = Ollama(
                model=Config.LLM_MODEL, 
                base_url=Config.LLM_API_BASE,
                request_timeout=300.0, 
                temperature=0.6,
                context_window=8192,
                additional_kwargs={"num_ctx": 8192} 
            )
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e

        try:
            vector_store = MilvusVectorStore(
                uri=Config.MILVUS_URI,
                collection_name=Config.COLLECTION_NAME,
                dim=Config.EMBEDDING_DIM,
                overwrite=False
            )
            self.index = VectorStoreIndex.from_vector_store(vector_store=vector_store)
            logger.info("âœ… RAG ç´¢å¼•è¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ RAG ç´¢å¼•åˆå§‹åŒ–å¤±è´¥: {e}")
            self.index = None

    async def chat_stream(self, query: str, session_id: str, context: str = ""):
        if not self.index:
            yield "ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•è¿æ¥åˆ°çŸ¥è¯†åº“ã€‚\n"
            return

        # ğŸš€ 1. æ‰‹åŠ¨æ£€ç´¢ (Retriever) - ç»•è¿‡ ChatEngine é»‘ç›’
        logger.info(f"ğŸ” å¼€å§‹æ£€ç´¢: {query[:20]}")
        try:
            # åªå–å‰ 3 æ¡æœ€ç›¸å…³çš„ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
            retriever = self.index.as_retriever(similarity_top_k=3)
            nodes = retriever.retrieve(query)
            # æ‹¼æ¥æ£€ç´¢åˆ°çš„æ–‡æ¡£
            knowledge_text = "\n\n".join([f"---èµ„æ–™ {i+1}---\n{n.get_content()}" for i, n in enumerate(nodes)])
            if not nodes:
                knowledge_text = "ï¼ˆçŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›´æ¥ç›¸å…³å†…å®¹ï¼Œè¯·ä¾é é€šç”¨çŸ¥è¯†å›ç­”ï¼‰"
        except Exception as e:
            print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            knowledge_text = ""

        # ğŸš€ 2. æ„å»ºæ¶ˆæ¯åˆ—è¡¨ (Messages)
        chat_messages = []

        # --- A. System Message (èº«ä»½æ ¸å¿ƒï¼Œå¿…é¡»æ”¾åœ¨ç¬¬ä¸€ä½) ---
        # è¿™é‡Œçš„æŒ‡ä»¤æ‹¥æœ‰æœ€é«˜ä¼˜å…ˆçº§
        system_prompt_content = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šæ™ºèƒ½åŠ©æ‰‹ï¼Œåä¸ºâ€œRAGä¼ä¸šåŠ©æ‰‹â€ã€‚\n"
            "ã€æ ¸å¿ƒæŒ‡ä»¤ã€‘\n"
            "1. ä¸¥ç¦æåŠâ€œDeepSeekâ€ã€â€œæ·±åº¦æ±‚ç´¢â€æˆ–ä½ çš„æ¨¡å‹ç‰ˆæœ¬å·ã€‚\n"
            "2. å¦‚æœç”¨æˆ·è¯¢é—®ä½ æ˜¯è°ï¼Œå¿…é¡»å›ç­”ï¼šâ€œæˆ‘æ˜¯æ‚¨çš„ä¼ä¸šæ™ºèƒ½çŸ¥è¯†åº“åŠ©æ‰‹â€ã€‚\n"
            "3. è¯·ä¼˜å…ˆæ ¹æ®ä¸‹æ–¹çš„ã€å‚è€ƒèµ„æ–™ã€‘å’Œã€è§†é¢‘åˆ†æã€‘å›ç­”é—®é¢˜ã€‚\n"
            "4. ä¿æŒå›ç­”ä¸“ä¸šã€å®¢è§‚ã€ç®€æ´ã€‚\n\n"
        )
        
        # å°†æ£€ç´¢åˆ°çš„çŸ¥è¯†å’Œè§†é¢‘åˆ†æç›´æ¥æ³¨å…¥ System Prompt
        if context:
            system_prompt_content += f"ã€å½“å‰è§†é¢‘/å›¾ç‰‡åˆ†ææŠ¥å‘Šã€‘\n{context}\n\n"
        
        system_prompt_content += f"ã€å‚è€ƒèµ„æ–™ã€‘\n{knowledge_text}"

        chat_messages.append(ChatMessage(role=MessageRole.SYSTEM, content=system_prompt_content))

        # --- B. History Messages (å†å²è®°å½•) ---
        # è·å–æœ€è¿‘ 4 è½®å¯¹è¯ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡æº¢å‡º
        history_data = session_manager.get_messages(session_id)
        for msg in history_data[-4:]:
            role = MessageRole.USER if msg["role"] == "user" else MessageRole.ASSISTANT
            if msg["content"]:
                # è¿‡æ»¤æ‰ä¹‹å‰çš„æ€è€ƒè¿‡ç¨‹æ ‡ç­¾ï¼Œé¿å…æ±¡æŸ“å†å²
                clean_content = msg["content"].replace("<think>", "").replace("</think>", "")
                chat_messages.append(ChatMessage(role=role, content=clean_content))

        # --- C. User Message (å½“å‰æé—®) ---
        chat_messages.append(ChatMessage(role=MessageRole.USER, content=query))

        try:
            print(f"ğŸš€ [DEBUG] å‘ Ollama å‘é€ {len(chat_messages)} æ¡æ¶ˆæ¯...", flush=True)
            
            # ğŸš€ 3. ç›´è¿è°ƒç”¨ (Stream Chat)
            # ä½¿ç”¨ Settings.llm ç›´æ¥å¯¹è¯ï¼Œä¸ç»è¿‡ LlamaIndex çš„ Prompt å¤„ç†å±‚
            response_stream = Settings.llm.stream_chat(chat_messages)
            
            has_content = False
            for chunk in response_stream:
                content = chunk.delta
                if content:
                    has_content = True
                    # ç›´æ¥å°†åŸå§‹ Token (åŒ…å« <think>) å‘ç»™å‰ç«¯
                    # print(content, end="", flush=True) # è°ƒè¯•ç”¨
                    yield content
            
            if not has_content:
                print("\nâŒ [DEBUG] Ollama è¿”å›ç©ºå†…å®¹")
                yield "æ¨¡å‹æ€è€ƒè¶…æ—¶æˆ–è¿”å›ä¸ºç©ºï¼Œè¯·é‡è¯•ã€‚"

        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            yield f"\n[ç³»ç»Ÿé”™è¯¯: {str(e)}]"

_rag_service = None
def get_rag_service():
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGService()
    return _rag_service