import logging
from llama_index.core import VectorStoreIndex, Settings
from llama_index.core.llms import ChatMessage, MessageRole
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.ollama import Ollama
from llama_index.vector_stores.milvus import MilvusVectorStore
from config import Config
from session_manager import session_manager
from prompts import build_system_prompt

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RAGService:
    def __init__(self):
        logger.info("ğŸ¤– åˆå§‹åŒ– RAG æœåŠ¡ (7B æé€Ÿç‰ˆ)...")
        
        try:
            logger.info(f"ğŸ”Œ åŠ è½½ Embedding: {Config.EMBEDDING_MODEL}")
            Settings.embed_model = HuggingFaceEmbedding(
                model_name=Config.EMBEDDING_MODEL,
                cache_folder=Config.MODEL_CACHE_DIR
            )
            
            logger.info(f"ğŸ§  è¿æ¥ LLM: {Config.LLM_MODEL}")
            # ğŸš€ã€æ ¸å¿ƒä¼˜åŒ–ã€‘æ‰‹åŠ¨è°ƒä¼˜ Ollama å‚æ•°
            Settings.llm = Ollama(
                model=Config.LLM_MODEL, 
                base_url=Config.LLM_API_BASE,
                request_timeout=300.0, 
                temperature=0.3, 
                context_window=Config.CONTEXT_WINDOW,
                additional_kwargs={
                    "num_ctx": Config.CONTEXT_WINDOW,
                    # ğŸ”¥ã€å…³é”®ã€‘é™åˆ¶æ¨ç†çº¿ç¨‹æ•°
                    # 32æ ¸ CPU å¹¶ä¸æ„å‘³ç€ num_thread=32 æœ€å¿«ã€‚
                    # é€šå¸¸ 8-16 ä¹‹é—´æ˜¯å†…å­˜å¸¦å®½çš„ç”œç‚¹ã€‚å»ºè®®è®¾ä¸º 12ã€‚
                    "num_thread": 12, 
                    "num_predict": -1,
                } 
            )
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e

        # ç§»é™¤ Rerankerï¼Œè¿½æ±‚æè‡´å“åº”é€Ÿåº¦
        self.reranker = None 

        try:
            vector_store = MilvusVectorStore(
                uri=Config.MILVUS_URI,
                collection_name=Config.COLLECTION_NAME,
                dim=Config.EMBEDDING_DIM,
                overwrite=False
            )
            self.index = VectorStoreIndex.from_vector_store(vector_store=vector_store)
            logger.info("âœ… RAG ç´¢å¼•è¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ RAG ç´¢å¼•åˆå§‹åŒ–å¤±è´¥: {e}")
            self.index = None

    async def chat_stream(self, query: str, session_id: str, context: str = ""):
        if not self.index:
            yield "ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•è¿æ¥åˆ°çŸ¥è¯†åº“ã€‚\n"
            return

        knowledge_text = ""
        
        # 1. ä¸Šä¸‹æ–‡äº’æ–¥ç­–ç•¥ (æœ‰è§†é¢‘å°±ä¸æŸ¥æ–‡æ¡£)
        if context:
            logger.info("ğŸ¥ æ£€æµ‹åˆ°è§†é¢‘ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡ RAG æ£€ç´¢ã€‚")
            knowledge_text = "" 
        else:
            logger.info(f"ğŸ” å¼€å§‹æ£€ç´¢çŸ¥è¯†åº“: {query[:20]}")
            try:
                # ğŸš€ã€ä¼˜åŒ–ã€‘åªå– Top 2
                # 7B æ¨¡å‹é˜…è¯»é€Ÿåº¦å¿«ï¼ŒTop 2 (çº¦ 700 tokens) å¯ä»¥åœ¨ 1-2ç§’å†…è¯»å®Œã€‚
                # æ—¢ä¿è¯äº†æœ‰è¶³å¤Ÿçš„èµ„æ–™ï¼Œåˆä¸ä¼šè®©é¢„å¤„ç†æ—¶é—´å¤ªé•¿ã€‚
                retriever = self.index.as_retriever(similarity_top_k=2)
                nodes = retriever.retrieve(query)
                
                if nodes:
                    knowledge_lines = []
                    for i, n in enumerate(nodes):
                        knowledge_lines.append(f"---èµ„æ–™ {i+1} (ä»…ä¾›å‚è€ƒ)---\n{n.get_content()}")
                    
                    if knowledge_lines:
                        knowledge_text = "\n\n".join(knowledge_lines)
                
                if not knowledge_text:
                    knowledge_text = "ï¼ˆæœªæ£€ç´¢åˆ°é«˜ç›¸å…³æ€§æ–‡æ¡£ï¼Œè¯·å¿½ç•¥æ­¤éƒ¨åˆ†ï¼‰"
                    
            except Exception as e:
                logger.error(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
                knowledge_text = ""

        # 2. æ„å»ºæ¶ˆæ¯
        chat_messages = []
        system_content = build_system_prompt(video_context=context, rag_context=knowledge_text)
        chat_messages.append(ChatMessage(role=MessageRole.SYSTEM, content=system_content))

        history_data = session_manager.get_messages(session_id)
        for msg in history_data[-4:]:
            role = MessageRole.USER if msg["role"] == "user" else MessageRole.ASSISTANT
            if msg["content"]:
                clean_content = msg["content"].replace("<think>", "").replace("</think>", "")
                chat_messages.append(ChatMessage(role=role, content=clean_content))

        chat_messages.append(ChatMessage(role=MessageRole.USER, content=query))

        # 3. å¼‚æ­¥æµå¼ç”Ÿæˆ
        try:
            logger.info(f"ğŸš€ å‘ Ollama å‘é€è¯·æ±‚ (Thread=12, Ctx={Config.CONTEXT_WINDOW})...")
            
            # ä½¿ç”¨ astream_chat ç¡®ä¿éé˜»å¡
            response_stream = await Settings.llm.astream_chat(chat_messages)
            
            has_content = False
            async for chunk in response_stream:
                content = chunk.delta
                if content:
                    has_content = True
                    yield content
            
            if not has_content:
                yield "æ¨¡å‹æ€è€ƒè¶…æ—¶æˆ–è¿”å›ä¸ºç©ºï¼Œè¯·é‡è¯•ã€‚"

        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå‡ºé”™: {e}")
            yield f"\n[ç³»ç»Ÿé”™è¯¯: {str(e)}]"

_rag_service = None
def get_rag_service():
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGService()
    return _rag_service